{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -so train.txt https://raw.githubusercontent.com/chaojiang06/chaojiang06.github.io/master/TA/spring2022_CS4650/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "device = 'mps'\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  7148\n",
      "Val Data:  1788\n",
      "Total tags:  44\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# Run some preprocessing code for our dataset. Don't modify anything in this cell.\n",
    "# ===========================================================================\n",
    "\n",
    "def load_tag_data(tag_file):\n",
    "    all_sentences = []\n",
    "    all_tags = []\n",
    "    sent = []\n",
    "    tags = []\n",
    "    with open(tag_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                all_sentences.append(sent)\n",
    "                all_tags.append(tags)\n",
    "                sent = []\n",
    "                tags = []\n",
    "            else:\n",
    "                word, tag, _ = line.strip().split()\n",
    "                sent.append(word)\n",
    "                tags.append(tag)\n",
    "    return all_sentences, all_tags\n",
    "\n",
    "train_sentences, train_tags = load_tag_data('train.txt')\n",
    "\n",
    "unique_tags = set([tag for tag_seq in train_tags for tag in tag_seq])\n",
    "\n",
    "# Create train-val split from train data\n",
    "train_val_data = list(zip(train_sentences, train_tags))\n",
    "random.shuffle(train_val_data)\n",
    "split = int(0.8 * len(train_val_data))\n",
    "training_data = train_val_data[:split]\n",
    "val_data = train_val_data[split:]\n",
    "\n",
    "print(\"Train Data: \", len(training_data))\n",
    "print(\"Val Data: \", len(val_data))\n",
    "print(\"Total tags: \", len(unique_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tags 44\n",
      "Vocab size 19122\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================================\n",
    "# Don't modify anything in this cell.\n",
    "# ===========================================================================\n",
    "\n",
    "word_to_idx = {}\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "\n",
    "tag_to_idx = {}\n",
    "for tag in unique_tags:\n",
    "    if tag not in tag_to_idx:\n",
    "        tag_to_idx[tag] = len(tag_to_idx)\n",
    "\n",
    "idx_to_tag = {}\n",
    "for tag in tag_to_idx:\n",
    "    idx_to_tag[tag_to_idx[tag]] = tag\n",
    "\n",
    "print(\"Total tags\", len(tag_to_idx))\n",
    "print(\"Vocab size\", len(word_to_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(sent, idx_mapping):\n",
    "    idxs = [idx_mapping[word] for word in sent]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1138, 4074])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepare_sequence([\"sometimes\", \"experiment\"], word_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhei/miniconda3/envs/ef/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 5.62MB/s]\n"
     ]
    }
   ],
   "source": [
    "# If you are interested in what other models are available, you can find a\n",
    "# list of model names here (e.g., roberta-base, bert-base-uncased):\n",
    "# https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizerFast\n",
    "bert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class POSDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer, max_len):\n",
    "    self.data = data\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Given an index, return the value in your training data (self.data). Make\n",
    "    sure the full output dict from self.tokenizer is returned, with an additional\n",
    "    value for your labels.\n",
    "\n",
    "    Remember! Your BERT tokenizer will give multiple tokens to words with the\n",
    "    same POS tag. We want the FIRST token be given the tag and all other tokens\n",
    "    to be given -100.\n",
    "\n",
    "    Hint: You may use the prepare_sequence() function from earlier sections\n",
    "    Hint: Our training data is already tokenized, so you may find the `is_split_into_words=True`\n",
    "      and `return_offsets_mapping=True` arguments helpful for getting the token offsets.\n",
    "    Hint: When using the tokenizer, you can also use padding='max_length' for [PAD]\n",
    "      tokens to be added for you.\n",
    "    \"\"\"\n",
    "    encoding = None\n",
    "\n",
    "    ### BEGIN YOUR CODE ###\n",
    "\n",
    "    # Get the sentence and POS tags\n",
    "    sentence, tags = self.data[index]\n",
    "    tags = prepare_sequence(tags, tag_to_idx).to(device)\n",
    "\n",
    "    # Use the BERT tokenizer (self.tokenizer) to encode the sentence. Make sure to\n",
    "    # truncate the sentence if it is longer than self.max_len, and pad the sentence if it\n",
    "    # is less than self.max_len.\n",
    "    encoding = self.tokenizer(sentence,\n",
    "                              is_split_into_words=True,\n",
    "                              return_offsets_mapping=True,\n",
    "                              padding='max_length',\n",
    "                              truncation=True,\n",
    "                              max_length=self.max_len)\n",
    "\n",
    "    # Create token labels, where the first token of each word is the POS tag, and\n",
    "    # all others are -100.\n",
    "    encoded_labels = torch.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100\n",
    "    i = 0\n",
    "    for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n",
    "      if mapping[0] == 0 and mapping[1] != 0:\n",
    "        encoded_labels[idx] = tags[i]\n",
    "        i += 1\n",
    "\n",
    "    # Add the token labels back to the tokenized dict\n",
    "    encoding['labels'] = torch.as_tensor(encoded_labels)\n",
    "\n",
    "    # Make sure both your encoded sentence, labels and attention mask are PyTorch tensors\n",
    "    encoding = {k: torch.as_tensor(v) for k, v in encoding.items()}\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your POSDataset class to create a train and test set\n",
    "MAX_LEN = 128\n",
    "\n",
    "# Further split your train data into train/test. You now have train/test/val.\n",
    "train_test_data, split = training_data, int(0.7 * len(training_data))\n",
    "random.shuffle(train_test_data)\n",
    "split_training_data, split_test_data = train_test_data[:split], train_test_data[split:]\n",
    "\n",
    "training_set = POSDataset(split_training_data, tokenizer, MAX_LEN)\n",
    "testing_set = POSDataset(split_test_data, tokenizer, MAX_LEN)\n",
    "validation_set = POSDataset(val_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1039,  1011, 11138,  9313,  3279,  1997,  2274, 16653,  1037,\n",
      "         3745,  1998,  9313,  5114,  1997,  2403, 16653,  1037,  3745,  2013,\n",
      "        23260,  3466,  1997,  9529,  2689,  1012,   102,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "tensor([-100,    3,   27,   25,   28,    3,   37,   10,   13,   42,    3,   20,\n",
      "          28,    3,   37,   10,   13,   42,    3,   37,   28,    3,   37,    3,\n",
      "           3,    5, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "# Print a few values from your Dataloader!\n",
    "print(training_set.__getitem__(0)['input_ids'])\n",
    "print(training_set.__getitem__(0)['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch dataloaders from the POSDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "testing_loader = DataLoader(testing_set, batch_size=64, shuffle=True)\n",
    "validating_loader = DataLoader(validation_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPOSTagging(DistilBertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        classifier_dropout = 0.5\n",
    "\n",
    "        self.bert = DistilBertModel(config)\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass through your model. Returns output logits for each POS\n",
    "        label and the loss (if labels is not None)\n",
    "\n",
    "        Hint: You may use nn.CrossEntropyLoss() to calculate your loss.\n",
    "        \"\"\"\n",
    "        loss, logits = None, None\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.loss(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        if loss is not None:\n",
    "          return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPOSTagging were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.bert.embeddings.LayerNorm.bias', 'distilbert.bert.embeddings.LayerNorm.weight', 'distilbert.bert.embeddings.position_embeddings.weight', 'distilbert.bert.embeddings.word_embeddings.weight', 'distilbert.bert.transformer.layer.0.attention.k_lin.bias', 'distilbert.bert.transformer.layer.0.attention.k_lin.weight', 'distilbert.bert.transformer.layer.0.attention.out_lin.bias', 'distilbert.bert.transformer.layer.0.attention.out_lin.weight', 'distilbert.bert.transformer.layer.0.attention.q_lin.bias', 'distilbert.bert.transformer.layer.0.attention.q_lin.weight', 'distilbert.bert.transformer.layer.0.attention.v_lin.bias', 'distilbert.bert.transformer.layer.0.attention.v_lin.weight', 'distilbert.bert.transformer.layer.0.ffn.lin1.bias', 'distilbert.bert.transformer.layer.0.ffn.lin1.weight', 'distilbert.bert.transformer.layer.0.ffn.lin2.bias', 'distilbert.bert.transformer.layer.0.ffn.lin2.weight', 'distilbert.bert.transformer.layer.0.output_layer_norm.bias', 'distilbert.bert.transformer.layer.0.output_layer_norm.weight', 'distilbert.bert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.bert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.bert.transformer.layer.1.attention.k_lin.bias', 'distilbert.bert.transformer.layer.1.attention.k_lin.weight', 'distilbert.bert.transformer.layer.1.attention.out_lin.bias', 'distilbert.bert.transformer.layer.1.attention.out_lin.weight', 'distilbert.bert.transformer.layer.1.attention.q_lin.bias', 'distilbert.bert.transformer.layer.1.attention.q_lin.weight', 'distilbert.bert.transformer.layer.1.attention.v_lin.bias', 'distilbert.bert.transformer.layer.1.attention.v_lin.weight', 'distilbert.bert.transformer.layer.1.ffn.lin1.bias', 'distilbert.bert.transformer.layer.1.ffn.lin1.weight', 'distilbert.bert.transformer.layer.1.ffn.lin2.bias', 'distilbert.bert.transformer.layer.1.ffn.lin2.weight', 'distilbert.bert.transformer.layer.1.output_layer_norm.bias', 'distilbert.bert.transformer.layer.1.output_layer_norm.weight', 'distilbert.bert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.bert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.bert.transformer.layer.2.attention.k_lin.bias', 'distilbert.bert.transformer.layer.2.attention.k_lin.weight', 'distilbert.bert.transformer.layer.2.attention.out_lin.bias', 'distilbert.bert.transformer.layer.2.attention.out_lin.weight', 'distilbert.bert.transformer.layer.2.attention.q_lin.bias', 'distilbert.bert.transformer.layer.2.attention.q_lin.weight', 'distilbert.bert.transformer.layer.2.attention.v_lin.bias', 'distilbert.bert.transformer.layer.2.attention.v_lin.weight', 'distilbert.bert.transformer.layer.2.ffn.lin1.bias', 'distilbert.bert.transformer.layer.2.ffn.lin1.weight', 'distilbert.bert.transformer.layer.2.ffn.lin2.bias', 'distilbert.bert.transformer.layer.2.ffn.lin2.weight', 'distilbert.bert.transformer.layer.2.output_layer_norm.bias', 'distilbert.bert.transformer.layer.2.output_layer_norm.weight', 'distilbert.bert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.bert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.bert.transformer.layer.3.attention.k_lin.bias', 'distilbert.bert.transformer.layer.3.attention.k_lin.weight', 'distilbert.bert.transformer.layer.3.attention.out_lin.bias', 'distilbert.bert.transformer.layer.3.attention.out_lin.weight', 'distilbert.bert.transformer.layer.3.attention.q_lin.bias', 'distilbert.bert.transformer.layer.3.attention.q_lin.weight', 'distilbert.bert.transformer.layer.3.attention.v_lin.bias', 'distilbert.bert.transformer.layer.3.attention.v_lin.weight', 'distilbert.bert.transformer.layer.3.ffn.lin1.bias', 'distilbert.bert.transformer.layer.3.ffn.lin1.weight', 'distilbert.bert.transformer.layer.3.ffn.lin2.bias', 'distilbert.bert.transformer.layer.3.ffn.lin2.weight', 'distilbert.bert.transformer.layer.3.output_layer_norm.bias', 'distilbert.bert.transformer.layer.3.output_layer_norm.weight', 'distilbert.bert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.bert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.bert.transformer.layer.4.attention.k_lin.bias', 'distilbert.bert.transformer.layer.4.attention.k_lin.weight', 'distilbert.bert.transformer.layer.4.attention.out_lin.bias', 'distilbert.bert.transformer.layer.4.attention.out_lin.weight', 'distilbert.bert.transformer.layer.4.attention.q_lin.bias', 'distilbert.bert.transformer.layer.4.attention.q_lin.weight', 'distilbert.bert.transformer.layer.4.attention.v_lin.bias', 'distilbert.bert.transformer.layer.4.attention.v_lin.weight', 'distilbert.bert.transformer.layer.4.ffn.lin1.bias', 'distilbert.bert.transformer.layer.4.ffn.lin1.weight', 'distilbert.bert.transformer.layer.4.ffn.lin2.bias', 'distilbert.bert.transformer.layer.4.ffn.lin2.weight', 'distilbert.bert.transformer.layer.4.output_layer_norm.bias', 'distilbert.bert.transformer.layer.4.output_layer_norm.weight', 'distilbert.bert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.bert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.bert.transformer.layer.5.attention.k_lin.bias', 'distilbert.bert.transformer.layer.5.attention.k_lin.weight', 'distilbert.bert.transformer.layer.5.attention.out_lin.bias', 'distilbert.bert.transformer.layer.5.attention.out_lin.weight', 'distilbert.bert.transformer.layer.5.attention.q_lin.bias', 'distilbert.bert.transformer.layer.5.attention.q_lin.weight', 'distilbert.bert.transformer.layer.5.attention.v_lin.bias', 'distilbert.bert.transformer.layer.5.attention.v_lin.weight', 'distilbert.bert.transformer.layer.5.ffn.lin1.bias', 'distilbert.bert.transformer.layer.5.ffn.lin1.weight', 'distilbert.bert.transformer.layer.5.ffn.lin2.bias', 'distilbert.bert.transformer.layer.5.ffn.lin2.weight', 'distilbert.bert.transformer.layer.5.output_layer_norm.bias', 'distilbert.bert.transformer.layer.5.output_layer_norm.weight', 'distilbert.bert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.bert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.classifier.bias', 'distilbert.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForPOSTagging.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=len(tag_to_idx)\n",
    ").to(device)\n",
    "\n",
    "MAX_GRAD_NORM = 10\n",
    "EPOCHS = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Currently allocated GPU memory: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB / {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    train_loss = 0\n",
    "    train_examples, train_steps = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "        ### BEGIN YOUR CODE ###\n",
    "\n",
    "        loss, _ = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Clip gradients (Not required, but helps training)\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "        train_steps += 1\n",
    "        train_examples += labels.size(0)\n",
    "\n",
    "    avg_train_loss = train_loss / train_steps\n",
    "    avg_val_loss, val_accuracy = evaluate_bert(model)\n",
    "\n",
    "    print(f\"Epoch: {epoch}/{EPOCHS}\\tAvg Train Loss: {avg_train_loss:.4f}\\tAvg Val Loss: {avg_val_loss:.4f}\\t Val Accuracy: {val_accuracy:.0f}\")\n",
    "\n",
    "def evaluate_bert(model):\n",
    "    correct, val_loss, val_examples = 0, 0, 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(validating_loader):\n",
    "            \"\"\"\n",
    "            Implement the evaluate method. Find the average validation loss\n",
    "            along with the validation accuracy.\n",
    "\n",
    "            Remember! You have labeled only the first token of each word. Make\n",
    "            sure you only calculate accuracy on values which are not -100.\n",
    "            \"\"\"\n",
    "            ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.long)\n",
    "\n",
    "            ### BEGIN YOUR CODE ###\n",
    "\n",
    "            loss, target_logits = model(input_ids=ids, attention_mask=mask, labels=labels)\n",
    "\n",
    "            # Compute training accuracy\n",
    "            flattened_targets = labels.view(-1)\n",
    "            active_logits = target_logits.view(-1, model.num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "            # Only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100\n",
    "\n",
    "            # Get the predicted labels\n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            pred = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "\n",
    "            # Get number of correct predictions\n",
    "            correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "\n",
    "            # Increase running total loss and the number of past valid samples\n",
    "            val_loss += loss\n",
    "            val_examples += labels.size(0)\n",
    "\n",
    "            ### END YOUR CODE ###\n",
    "\n",
    "    val_accuracy = 100 * correct / val_examples\n",
    "    avg_val_loss = val_loss / val_examples\n",
    "    return avg_val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1\tAvg Train Loss: 1.1670\tAvg Val Loss: 0.0031\t Val Accuracy: 83\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model, sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, generate a full prediction of POS tags.\n",
    "\n",
    "    In this case, you are given a full sentence (not array of tokens), so you\n",
    "    will need to use your tokenizer differently.\n",
    "\n",
    "    Return your prediction in the format:\n",
    "      [(token 1, POS prediction 1), (token 2, POS prediction 2), ...]\n",
    "\n",
    "    E.g., \"The imperatives that\" => [('the', 'DT'), ('imperative', 'NNS'), ('that', 'WDT')]\n",
    "    \"\"\"\n",
    "    prediction = []\n",
    "\n",
    "    ### BEGIN YOUR CODE ###\n",
    "\n",
    "    reversed_tag_to_idx = {v: k for k, v in tag_to_idx.items()}\n",
    "\n",
    "    inputs = tokenizer(sentence,\n",
    "                       return_offsets_mapping=True,\n",
    "                       padding='max_length',\n",
    "                       truncation=True,\n",
    "                       max_length=MAX_LEN,\n",
    "                       return_tensors=\"pt\")\n",
    "\n",
    "    ids = inputs[\"input_ids\"].to(device)\n",
    "    mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    outputs = model(input_ids=ids, attention_mask=mask)\n",
    "    logits = outputs[0]\n",
    "\n",
    "    active_logits = logits.view(-1, model.num_labels)\n",
    "    flattened_predictions = torch.argmax(active_logits, axis=1)\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
    "    token_predictions = [reversed_tag_to_idx[i] for i in flattened_predictions.cpu().numpy()]\n",
    "    wp_preds = list(zip(tokens, token_predictions))\n",
    "\n",
    "    for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
    "      if mapping[0] != 0 or mapping[1] != 0:\n",
    "        prediction += [token_pred]\n",
    "      else:\n",
    "        continue\n",
    "\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('imperative', 'NN'), ('##s', 'NN'), ('that', 'IN'), ('can', 'MD'), ('be', 'VB'), ('obeyed', 'VBG'), ('by', 'IN'), ('a', 'DT'), ('machine', 'NN'), ('that', 'IN'), ('has', 'VBZ'), ('no', 'DT'), ('limbs', 'VB'), ('are', 'VBP'), ('bound', 'NN'), ('to', 'TO'), ('be', 'VB'), ('of', 'IN'), ('a', 'DT'), ('rather', 'RB'), ('intellectual', 'JJ'), ('character', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The imperatives that can be obeyed by a machine that has no limbs are bound to be of a rather intellectual character.\"\n",
    "print(generate_prediction(model, sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ef",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
